#+title: NLP Pipeline
#+subtitle: Natural Language Processing
#+author: Alberto Valdez
#+SETUPFILE: ../config/org-theme-alt.config
#+SETUPFILE: ../config/org-header.config

* NLP Pipeline

** Google Collab

#+begin_src python :eval no
import os
# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version
# For example:
# spark_version = 'spark-3.0.3'
spark_version = 'spark-3.3.0'
os.environ['SPARK_VERSION']=spark_version

# Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz
!tar xf $SPARK_VERSION-bin-hadoop3.tgz
!pip install -q findspark

# Set Environment Variables
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop3"

# Start a SparkSession
import findspark
findspark.init()
#+end_src

** Spark

Start Spark session.

#+begin_src python :exports code
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Tokens").getOrCreate()
#+end_src

#+RESULTS[d164f0a23f4f79bd6a6ac5c40053ab91b60aa1fc]:
#+begin_example
#+end_example

Import and use Tokenizer.

#+begin_src python
from pyspark.ml.feature import Tokenizer
tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
print(tokenizer)
#+end_src

#+RESULTS[0995d0c5a7948fa5f329e9b6b07c72867080855c]:
#+begin_example
Tokenizer_1604cacf6fb2
#+end_example

Let's say we have the following dataframe.

#+begin_src python
df = spark.createDataFrame(
    data=[
        (1, "Spark is great"),
        (2, "We are learning Spark"),
        (3, "Spark is better than hadoop")
    ],
    schema=["id", "sentence"]
)
df.show(5)
#+end_src

#+RESULTS[61a2d04f261ca2a0c4d9e526eca63256b7f79bdb]:
#+begin_example
+---+--------------------+
| id|            sentence|
+---+--------------------+
|  1|      Spark is great|
|  2|We are learning S...|
|  3|Spark is better t...|
+---+--------------------+
#+end_example

The tokenizer uses a transform method that takes a DataFrame as input.

#+begin_src python
tokenized_df = tokenizer.transform(df)
tokenized_df.show(truncate=False)
#+end_src

#+RESULTS[37a9a3f94de61febd15a8710902e018a13c84fd4]:
#+begin_example
+---+---------------------------+---------------------------------+
|id |sentence                   |words                            |
+---+---------------------------+---------------------------------+
|1  |Spark is great             |[spark, is, great]               |
|2  |We are learning Spark      |[we, are, learning, spark]       |
|3  |Spark is better than hadoop|[spark, is, better, than, hadoop]|
+---+---------------------------+---------------------------------+
#+end_example

** User-defined Functions

User-defined functions (UDFs) are functions created by the user to add custom output columns.

Create a function to return the length of a list.

#+begin_src python
def word_list_length(word_list):
    return len(word_list)
#+end_src

#+RESULTS[c16d8dd7fcbb98a9e1d5fac13b3262219404fc75]:
#+begin_example
#+end_example

Next, we'll import the udf function, the col function to select a column to be passed into a function.

#+begin_src python
from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType

count_tokens = udf(word_list_length, IntegerType())
#+end_src

#+RESULTS[7b074b6782bcf58cc45ceea018bdd6aa4e0a8e8d]:
#+begin_example
#+end_example

The udf will take in the name of the function as a parameter and the output data type, which is the IntegerType that we just imported.

We repeat the process now. This time we include the function we previously created to get more information about the token result.

#+begin_src python
tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
tokenized_df = tokenizer.transform(df)
tokenized_df.withColumn("tokens", count_tokens(col("words"))).show(truncate=False)
#+end_src

#+RESULTS[3ad9169d6d9960cfb6f3d90a587593779018ef1f]:
#+begin_example
+---+---------------------------+---------------------------------+------+
|id |sentence                   |words                            |tokens|
+---+---------------------------+---------------------------------+------+
|1  |Spark is great             |[spark, is, great]               |3     |
|2  |We are learning Spark      |[we, are, learning, spark]       |4     |
|3  |Spark is better than hadoop|[spark, is, better, than, hadoop]|5     |
+---+---------------------------+---------------------------------+------+
#+end_example

** Stop Words

Stop words are words with little to no linguistic value in NLP. Removing them from the data can improve accuracy of the language model. Examples are: "a", "the", "and".

#+begin_src python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("StopWords").getOrCreate()
#+end_src

#+RESULTS[eb05ba5f81c190e5257b6a6c3893689a4bda151c]:
#+begin_example
22/10/20 13:38:42 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
#+end_example

We make a df.

#+begin_src python
df = spark.createDataFrame([
    (0, ["Big", "data", "is", "super", "fun"]),
    (1, ["This", "is", "going", "to", "be", "epic"])
], ["id", "raw"])
df.show(truncate=False)
#+end_src

#+RESULTS[80fb2a764c11f3ca52bae85e9dc6db1192bb5b48]:
#+begin_example
+---+-------------------------------+
|id |raw                            |
+---+-------------------------------+
|0  |[Big, data, is, super, fun]    |
|1  |[This, is, going, to, be, epic]|
+---+-------------------------------+
#+end_example

Now we import and run the word remover.

#+begin_src python
from pyspark.ml.feature import StopWordsRemover

remover = StopWordsRemover(inputCol="raw", outputCol="filtered")
remover.transform(df).show(truncate=False)
#+end_src

#+RESULTS[27518e960a99043cb26a646c741b7b21244cca49]:
#+begin_example
+---+-------------------------------+-----------------------+
|id |raw                            |filtered               |
+---+-------------------------------+-----------------------+
|0  |[Big, data, is, super, fun]    |[Big, data, super, fun]|
|1  |[This, is, going, to, be, epic]|[going, epic]          |
+---+-------------------------------+-----------------------+
#+end_example

** Term Frequency-Inverse

Term Frequency-Inverse Document Frequency Weight (TF-IDF) is a statistical weight showing the importance of a word in a document. TF measures the frequency of a word occurring in a document. IDF measures the significance of a word across a set of documents. Multiplyng the two fives us the TF-IDF.

Example.

If "Python" appears 20 times in a 1,000-word article, the TF weight would be 20 divided by 1,000 which equals 0.02. Assume that this article resides in a database with other articles, totaling 1,000,000 articles.

DF takes the number of documents that contain the word Python and compares to the total number of documents, then takes the logarithm of the results. If Python is mentioned in 1000 articles and the total amount of articles in the database is 1,000,000, the IDF would be the log of the total number of articles divided by the number of articles that contain the word Python.

#+begin_quote
IDF = log(total articles / articles that contain the word Python)
IDF = log(1,000,000 / 1000) = 3
#+end_quote

Now that we have both numbers, we can determine the TF-IDF which is the multiple of the two.

#+begin_quote
TF * IDF = 0.2 * 3 = .6
#+end_quote

There are two ways to convert text to numeric data.

- A =CountVectorizer= indexes the words accross all documents and returns a vector of word counts correponding to the indeces. They are assigned in descending order of frequency. For example, the word with the highest frequency across all documents will be given an index of 0, and the word with the lowest frequency will have an index equal to the number of words in the text.

- A =HashingTF= converts words to numeric IDs, where the same words are assigned to the same ID and counted then a vector is returned.

** TF-IDF Python

We are going to import the previous Tokenizer classes plus the HashingTF.

#+begin_src python
from pyspark.sql import SparkSession
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover

spark = SparkSession.builder.appName("TF-IDF").getOrCreate()
#+end_src

#+RESULTS[5613afa6bfab6a030444957c957c50466dbbae21]:
#+begin_example
22/10/20 14:00:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
#+end_example

Then we will load data from the web.

#+begin_src python
from pyspark import SparkFiles
url ="https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-online/module_16/airlines.csv"
spark.sparkContext.addFile(url)
df = spark.read.csv(SparkFiles.get("airlines.csv"), sep=",", header=True)

df.show()
#+end_src

#+RESULTS[e913766c1850e457cdab79f11bf372799f762465]:
#+begin_example
+--------------------+
|      Airline Tweets|
+--------------------+
|@VirginAmerica pl...|
|@VirginAmerica se...|
|@VirginAmerica do...|
|@VirginAmerica Ar...|
|@VirginAmerica aw...|
+--------------------+
#+end_example

We tokenize the dataframe.

#+begin_src python
tokened = Tokenizer(inputCol="Airline Tweets", outputCol="words")
tokened_transformed = tokened.transform(df)
tokened_transformed.show()
#+end_src

#+RESULTS[8771b2078119f1e870cb19d4b690dd9c341fc593]:
#+begin_example
+--------------------+--------------------+
|      Airline Tweets|               words|
+--------------------+--------------------+
|@VirginAmerica pl...|[@virginamerica, ...|
|@VirginAmerica se...|[@virginamerica, ...|
|@VirginAmerica do...|[@virginamerica, ...|
|@VirginAmerica Ar...|[@virginamerica, ...|
|@VirginAmerica aw...|[@virginamerica, ...|
+--------------------+--------------------+
#+end_example

Then we remove stop-words.

#+begin_src python
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
removed_frame = remover.transform(tokened_transformed)
removed_frame.show(truncate=False)
#+end_src

#+RESULTS[8b0ed18a3db6c0433a5760813f0456a3bdbc07d4]:
#+begin_example
+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+
|Airline Tweets                                                                                                                         |words                                                                                                                                                          |filtered                                                                                       |
+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+
|@VirginAmerica plus you've added commercials to the experience... tacky.                                                               |[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |[@virginamerica, plus, added, commercials, experience..., tacky.]                              |
|@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|[@virginamerica, seriously, pay, $30, flight, seats, playing., really, bad, thing, flying, va] |
|@VirginAmerica do you miss me? Don't worry we'll be together very soon.                                                                |[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |[@virginamerica, miss, me?, worry, together, soon.]                                            |
|@VirginAmerica Are the hours of operation for the Club at SFO that are posted online current?                                          |[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |[@virginamerica, hours, operation, club, sfo, posted, online, current?]                        |
|@VirginAmerica awaiting my return phone call, just would prefer to use your online self-service option :(                              |[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |[@virginamerica, awaiting, return, phone, call,, prefer, use, online, self-service, option, :(]|
+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+
#+end_example

Now we can use the HashingTF function. It takes input and output column plus a parameter to specify the buckets for the split words. This number must be higher than the number of unique words.

#+begin_src python
hashing = HashingTF(
    inputCol="filtered",
    outputCol="hashedValues",
    numFeatures=pow(2, 18),
)
hashed_df = hashing.transform(removed_frame)
hashed_df.show(truncate=False)
#+end_src

#+RESULTS[4013b8c11594967f1e177ddf34647f634b47c4cd]:
#+begin_example
+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
|Airline Tweets                                                                                                                         |words                                                                                                                                                          |filtered                                                                                       |hashedValues                                                                                                                               |
+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
|@VirginAmerica plus you've added commercials to the experience... tacky.                                                               |[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |[@virginamerica, plus, added, commercials, experience..., tacky.]                              |(262144,[1419,99916,168274,171322,186669,256498],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                |
|@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|[@virginamerica, seriously, pay, $30, flight, seats, playing., really, bad, thing, flying, va] |(262144,[30053,44911,70065,94512,99549,145380,166947,171322,178915,229264,237593,239713],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|
|@VirginAmerica do you miss me? Don't worry we'll be together very soon.                                                                |[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |[@virginamerica, miss, me?, worry, together, soon.]                                            |(262144,[107065,117975,147224,171322,200547,232735],[1.0,1.0,1.0,1.0,1.0,1.0])                                                             |
|@VirginAmerica Are the hours of operation for the Club at SFO that are posted online current?                                          |[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |[@virginamerica, hours, operation, club, sfo, posted, online, current?]                        |(262144,[9641,50671,83962,96266,171322,181964,192171,220194],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                            |
|@VirginAmerica awaiting my return phone call, just would prefer to use your online self-service option :(                              |[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |[@virginamerica, awaiting, return, phone, call,, prefer, use, online, self-service, option, :(]|(262144,[6122,50509,50671,67607,98717,121947,128077,171322,225517,234877,261675],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |
+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
#+end_example

The hashedValues will contain the index and term frequency of the corresponding index for each word.

Now we can plug the data into an IDFModel. It will scale the values while down-weighting based on document frequency.

#+begin_src python
idf = IDF(inputCol="hashedValues", outputCol="features")
idfModel = idf.fit(hashed_df)
rescaledData = idfModel.transform(hashed_df)
rescaledData.select("words", "features").show(truncate=False)
#+end_src

#+RESULTS[a086c3a515fda7f570cc34f730bbb1528733c8f3]:
#+begin_example
22/10/20 14:07:53 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB
+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|words                                                                                                                                                          |features                                                                                                                                                                                                                                                                                                        |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]                                                                             |(262144,[1419,99916,168274,171322,186669,256498],[1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098])                                                                                                                                                          |
|[@virginamerica, seriously, would, pay, $30, a, flight, for, seats, that, didn't, have, this, playing., it's, really, the, only, bad, thing, about, flying, va]|(262144,[30053,44911,70065,94512,99549,145380,166947,171322,178915,229264,237593,239713],[1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098])|
|[@virginamerica, do, you, miss, me?, don't, worry, we'll, be, together, very, soon.]                                                                           |(262144,[107065,117975,147224,171322,200547,232735],[1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098])                                                                                                                                                       |
|[@virginamerica, are, the, hours, of, operation, for, the, club, at, sfo, that, are, posted, online, current?]                                                 |(262144,[9641,50671,83962,96266,171322,181964,192171,220194],[1.0986122886681098,0.6931471805599453,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098])                                                                                                        |
|[@virginamerica, awaiting, my, return, phone, call,, just, would, prefer, to, use, your, online, self-service, option, :(]                                     |(262144,[6122,50509,50671,67607,98717,121947,128077,171322,225517,234877,261675],[1.0986122886681098,1.0986122886681098,0.6931471805599453,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,0.0,1.0986122886681098,1.0986122886681098,1.0986122886681098])                           |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
#+end_example

The result of the TF-IDF will be the floating point numbers in the features column.

* Pipeline Example
** Load Pyspark

We are going to go over the complete NLP pipeline.

#+begin_src python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Yelp_NLP").getOrCreate()
#+end_src

#+RESULTS[39398f6eed2932ac335152f39ba03586d9ee24b8]:
#+begin_example
22/10/20 14:10:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
#+end_example

** Load data from the web.

#+begin_src python
from pyspark import SparkFiles
url ="https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-online/module_16/yelp_reviews.csv"
spark.sparkContext.addFile(url)
df = spark.read.csv(SparkFiles.get("yelp_reviews.csv"), sep=",", header=True)

df.show(5)
#+end_src

#+RESULTS[27abe253f4b92dde2d017c0356d3e2af458bdee1]:
#+begin_example
22/10/20 14:10:54 WARN SparkContext: The path https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-online/module_16/yelp_reviews.csv has been added already. Overwriting of added paths is not supported in the current version.
+--------+--------------------+
|   class|                text|
+--------+--------------------+
|positive|Wow... Loved this...|
|negative|  Crust is not good.|
|negative|Not tasty and the...|
|positive|Stopped by during...|
|positive|The selection on ...|
+--------+--------------------+
only showing top 5 rows
#+end_example

** Import Functions and Features

#+begin_src python
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer
#+end_src

#+RESULTS[e9bb1e3f8958dd294ede91e93348f2e6df16d122]:
#+begin_example
#+end_example

** Get text length

#+begin_src python
from pyspark.sql.functions import length
data_df = df.withColumn(
    'length',
    length(df['text'])
)
data_df.show()
#+end_src

#+RESULTS[07107c1fcee01e076d748a289fa38106e53d9453]:
#+begin_example
+--------+--------------------+------+
|   class|                text|length|
+--------+--------------------+------+
|positive|Wow... Loved this...|    24|
|negative|  Crust is not good.|    18|
|negative|Not tasty and the...|    41|
|positive|Stopped by during...|    87|
|positive|The selection on ...|    59|
|negative|Now I am getting ...|    46|
|negative|Honeslty it didn'...|    37|
|negative|The potatoes were...|   111|
|positive|The fries were gr...|    25|
|positive|      A great touch.|    14|
|positive|Service was very ...|    24|
|negative|  Would not go back.|    18|
|negative|The cashier had n...|    99|
|positive|I tried the Cape ...|    59|
|negative|I was disgusted b...|    62|
|negative|I was shocked bec...|    50|
|positive| Highly recommended.|    19|
|negative|Waitress was a li...|    38|
|negative|This place is not...|    51|
|negative|did not like at all.|    20|
+--------+--------------------+------+
only showing top 20 rows
#+end_example

** Create Transformations

We are going to create all the features to the dataset. These are all the procedures that we will run in our dataframe.

#+begin_src python
pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')

tokenizer = Tokenizer(inputCol="text", outputCol="token_text")

stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')

hashingTF = HashingTF(inputCol="stop_tokens", outputCol='hash_token')

idf = IDF(inputCol='hash_token', outputCol='idf_token')
#+end_src

#+RESULTS[548b8773fb8bde68c512cfd6e5a8b328a2af6765]:
#+begin_example
#+end_example

** Creating the Feature Vector

Vector containing the output from the IDFModel. This will combine all the raw features to train the ML model that we will be using.

#+begin_src python
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.linalg import Vector

clean_up = VectorAssembler(
    inputCols=['idf_token', 'length'],
    outputCol='features'
)
print(clean_up)
#+end_src

#+RESULTS[7847b5a8ecaf065c5e1ef54b7ee0243a4b47b416]:
#+begin_example
VectorAssembler_86d8a9460eb3
#+end_example

** Creating the Pipeline

Now we combine all the steps via the =Pipeline= class from pyspark ml module.

#+begin_src python
from pyspark.ml import Pipeline

data_prep_pipeline = Pipeline(stages=[
        pos_neg_to_num,
        tokenizer,
        stopremove,
        hashingTF,
        idf,
        clean_up
])
print(data_prep_pipeline)
#+end_src

#+RESULTS[d66592b3e89c51a2c239c16e459bee6313bae7d6]:
#+begin_example
Pipeline_0e5518cfa9ea
#+end_example

** Run the Pipeline

Now we fit and transform the pipeline.

#+begin_src python
cleaner = data_prep_pipeline.fit(data_df)
cleaned = cleaner.transform(data_df)
cleaned.select(['label', 'features']).show(5)
#+end_src

#+RESULTS[16228bffd453cf5f930c2cc6ce99708d535121c7]:
#+begin_example
22/10/20 14:21:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB
+-----+--------------------+
|label|            features|
+-----+--------------------+
|  1.0|(262145,[177414,2...|
|  0.0|(262145,[49815,23...|
|  0.0|(262145,[109649,1...|
|  1.0|(262145,[53101,68...|
|  1.0|(262145,[15370,77...|
+-----+--------------------+
only showing top 5 rows
#+end_example

** Train the Model

We will split the data for training and testing so we can evaluate our model's performance after training.

#+begin_src python
training, testing = cleaned.randomSplit([0.7, 0.3], 21)
#+end_src

#+RESULTS[843a44350d95ced3196c5bea4ee38dc31b08bed2]:
#+begin_example
#+end_example

The ML model we'll use is Naive Bayes, which we'll import and then fit the model using the training dataset. Naive Bayes is a group of classifier algorithms based on Bayes' theorem. Bayes theorem provides a way to determine the probability of an event based on new conditions or information that might be related to the event.

#+begin_src python :results none
from pyspark.ml.classification import NaiveBayes
nb = NaiveBayes()
predictor = nb.fit(training)
#+end_src

The last column "prediction" will have the results. It will indicate 1.0 if the model thinks that the review is negative and 0.0 if it thinks its positive. Future datasets can be run with this model and determine wether a review was positive or negative without having already supplied the data.

#+begin_src python
test_results = predictor.transform(testing)
test_results.show(5)
#+end_src

#+RESULTS[34b98fb1f65c626b94791449681dcc6e73dbcc12]:
#+begin_example
22/10/20 14:29:55 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB
[Stage 73:>                                      (0 + 1) / 1]                                                             +--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+
|   class|                text|length|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|
+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+
|negative|"The burger... I ...|    86|  0.0|["the, burger...,...|["the, burger...,...|(262144,[20298,21...|(262144,[20298,21...|(262145,[20298,21...|[-820.60780566975...|[0.99999999999995...|       0.0|
|negative|              #NAME?|     6|  0.0|            [#name?]|            [#name?]|(262144,[197050],...|(262144,[197050],...|(262145,[197050,2...|[-73.489435340867...|[0.07515735596910...|       1.0|
|negative|After I pulled up...|    83|  0.0|[after, i, pulled...|[pulled, car, wai...|(262144,[65645,71...|(262144,[65645,71...|(262145,[65645,71...|[-620.40646705112...|[1.0,1.9205984091...|       0.0|
|negative|Also, I feel like...|    58|  0.0|[also,, i, feel, ...|[also,, feel, lik...|(262144,[61899,66...|(262144,[61899,66...|(262145,[61899,66...|[-528.59562125515...|[0.99999999994873...|       0.0|
|negative|Anyway, I do not ...|    44|  0.0|[anyway,, i, do, ...|[anyway,, think, ...|(262144,[132270,1...|(262144,[132270,1...|(262145,[132270,1...|[-334.09599709326...|[0.99999999994185...|       0.0|
+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+
only showing top 5 rows
#+end_example

** Evaluate the Model

How useful is this model? Should we just blindly trust that it will be right every time?

We will use the BinaryClassificationEvaluator to determine how accurate is our model.

#+begin_src python
from pyspark.ml.evaluation import BinaryClassificationEvaluator

acc_eval = BinaryClassificationEvaluator(
    labelCol='label',
    rawPredictionCol='prediction'
)
acc = acc_eval.evaluate(test_results)
print(f"Accuracy result is: {acc}")
#+end_src

#+RESULTS[161e394d40e6367faa079a97e77f4fdd4ff24e54]:
#+begin_example
22/10/20 14:34:29 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB
[Stage 77:>                                      (0 + 1) / 1]                                                             Accuracy result is: 0.7002978406552495
#+end_example

The model is not very accurate but may be good enough for a start. We can always train it with more data and change features to try to get better results.
